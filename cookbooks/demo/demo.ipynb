{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1ec745-507f-4c67-802a-f0b33fc8b1a9",
   "metadata": {},
   "source": [
    "# Mythic AI Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34176e1-b9b1-4c40-9e90-776a455c4fb7",
   "metadata": {},
   "source": [
    "### Download TTS engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62b54f41-993d-4c47-9fc8-4e46f497634b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded: en_US-amy-low\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p runtimes && cd runtimes && python3 -m piper.download_voices en_US-amy-low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e2c41a-3869-4ad7-9864-f183f707d4b2",
   "metadata": {},
   "source": [
    "### Import neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f12106d-5a69-4067-a45c-4c95370117de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "from piper import PiperVoice\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0af1167f-2e64-4271-aa35-0a22f854db84",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to load model from file: /home/lhqezio/.cache/huggingface/hub/models--bartowski--mistralai_Voxtral-Mini-3B-2507-GGUF/snapshots/1d0032d997b7f72804d356fc790486e6d697cea5/./mistralai_Voxtral-Mini-3B-2507-Q4_K_S.gguf",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# This will auto-download the file from HF (cached), then load it.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m llm = \u001b[43mLlama\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbartowski/mistralai_Voxtral-Mini-3B-2507-GGUF\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmistralai_Voxtral-Mini-3B-2507-Q4_K_S.gguf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Optional performance knobs:\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8192\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_gpu_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# set >0 or -1 for full offload if you have CUDA/Metal build\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Use like normal:\u001b[39;00m\n\u001b[32m     12\u001b[39m out = llm.create_chat_completion(\n\u001b[32m     13\u001b[39m     messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mGive me two sentences about Voxtral Mini.\u001b[39m\u001b[33m\"\u001b[39m}],\n\u001b[32m     14\u001b[39m     max_tokens=\u001b[32m200\u001b[39m,\n\u001b[32m     15\u001b[39m     temperature=\u001b[32m0.2\u001b[39m,\n\u001b[32m     16\u001b[39m     top_p=\u001b[32m0.95\u001b[39m,\n\u001b[32m     17\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mythic/cookbooks/demo/.venv/lib/python3.12/site-packages/llama_cpp/llama.py:2361\u001b[39m, in \u001b[36mLlama.from_pretrained\u001b[39m\u001b[34m(cls, repo_id, filename, additional_files, local_dir, local_dir_use_symlinks, cache_dir, **kwargs)\u001b[39m\n\u001b[32m   2358\u001b[39m     model_path = os.path.join(local_dir, filename)\n\u001b[32m   2360\u001b[39m \u001b[38;5;66;03m# loading the first file of a sharded GGUF loads all remaining shard files in the subfolder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2361\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   2362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2363\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2364\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mythic/cookbooks/demo/.venv/lib/python3.12/site-packages/llama_cpp/llama.py:374\u001b[39m, in \u001b[36mLlama.__init__\u001b[39m\u001b[34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_ubatch, n_threads, n_threads_batch, rope_scaling_type, pooling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, flash_attn, op_offload, swa_full, no_perf, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, type_k, type_v, spm_infill, verbose, **kwargs)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(model_path):\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    372\u001b[39m \u001b[38;5;28mself\u001b[39m._model = \u001b[38;5;28mself\u001b[39m._stack.enter_context(\n\u001b[32m    373\u001b[39m     contextlib.closing(\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m         \u001b[43minternals\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m     )\n\u001b[32m    380\u001b[39m )\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# Override tokenizer\u001b[39;00m\n\u001b[32m    383\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer_ = tokenizer \u001b[38;5;129;01mor\u001b[39;00m LlamaTokenizer(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Mythic/cookbooks/demo/.venv/lib/python3.12/site-packages/llama_cpp/_internals.py:58\u001b[39m, in \u001b[36mLlamaModel.__init__\u001b[39m\u001b[34m(self, path_model, params, verbose)\u001b[39m\n\u001b[32m     53\u001b[39m     model = llama_cpp.llama_model_load_from_file(\n\u001b[32m     54\u001b[39m         \u001b[38;5;28mself\u001b[39m.path_model.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m.params\n\u001b[32m     55\u001b[39m     )\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to load model from file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m vocab = llama_cpp.llama_model_get_vocab(model)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Failed to load model from file: /home/lhqezio/.cache/huggingface/hub/models--bartowski--mistralai_Voxtral-Mini-3B-2507-GGUF/snapshots/1d0032d997b7f72804d356fc790486e6d697cea5/./mistralai_Voxtral-Mini-3B-2507-Q4_K_S.gguf"
     ]
    }
   ],
   "source": [
    "# This will auto-download the file from HF (cached), then load it.\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF\",\n",
    "    filename=\"mistralai_Voxtral-Mini-3B-2507-Q4_K_S.gguf\",\n",
    "    # Optional performance knobs:\n",
    "    n_ctx=8192,\n",
    "    n_gpu_layers=-1,       # set >0 or -1 for full offload if you have CUDA/Metal build\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Use like normal:\n",
    "out = llm.create_chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Give me two sentences about Voxtral Mini.\"}],\n",
    "    max_tokens=200,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    ")\n",
    "print(out[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363cfc2f-a74d-48aa-a806-7318bb8ead71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
